{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module66 ML Logistic Regression Assignment2"
      ],
      "metadata": {
        "id": "K0Olp1WiRR9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
      ],
      "metadata": {
        "id": "rj2s8Ve2Q9gN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. **Purpose:** Grid Search with Cross-Validation (CV) is used to find the optimal hyperparameters for a machine learning model by systematically searching through a predefined set of values.\n",
        "\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "1.) Specify a grid of hyperparameter combinations to test.\n",
        "\n",
        "2.) For each combination:\n",
        "\n",
        "Perform k-fold cross-validation.\n",
        "\n",
        "Calculate the performance metric for each fold.\n",
        "\n",
        "3.) Choose the combination with the best cross-validated performance.\n",
        "```\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
        "\n",
        "# Create and run GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "CZZIwmr-RSqY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u60_wxn7RTLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
        "one over the other?"
      ],
      "metadata": {
        "id": "QalegTYnQ-pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. The differemnce between Grid search CV and Randomize Search CV according to:\n",
        "\n",
        "**1.) Search space**\n",
        "\n",
        "GridSearchCV - Exhaustively tests all parameter combinations.\n",
        "\n",
        "RandomizedSearchCV - Randomly selects combinations from the grid.\n",
        "\n",
        "**2.) Efficiency**\n",
        "\n",
        "GridSearchCV - Computationally expensive for large grids.\n",
        "\n",
        "RandomizedSearchCV - Faster, especially for large parameter spaces.\n",
        "\n",
        "**3.) Optimal usecase**\n",
        "\n",
        "GridSearchCV - Small, well-defined parameter grids.\n",
        "\n",
        "RandomizedSearchCV - Large, less-defined parameter grids.\n",
        "\n",
        "\n",
        "\n",
        "## When to Choose:\n",
        "\n",
        "1.) Use Grid Search for small parameter grids where exhaustive testing is feasible.\n",
        "\n",
        "2.) Use Randomized Search for large parameter grids or when you need faster results."
      ],
      "metadata": {
        "id": "hfVmy51IRT4-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkCJNL_NRUd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
      ],
      "metadata": {
        "id": "Jw4AzgwHRBxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. **Definition:**\n",
        "\n",
        "Data leakage occurs when information from outside the training dataset is inadvertently used to build the model, leading to over-optimistic performance metrics.\n",
        "\n",
        "**Problem:**\n",
        "It causes the model to perform well during training/validation but poorly on unseen data.\n",
        "\n",
        "**Example:**\n",
        "Predicting loan defaults, where the dataset includes the target variable (e.g., loan repayment status) in a feature column."
      ],
      "metadata": {
        "id": "WPc_l3ruRVbl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oJ03FoWRWAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How can you prevent data leakage when building a machine learning model?"
      ],
      "metadata": {
        "id": "6wyLHKEMRC3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. Ways to prevent data leakage when building a ML model are:\n",
        "\n",
        "**1.) Split Data Properly:** Ensure test data is not used during training or feature engineering.\n",
        "\n",
        "**2.)Feature Engineering:** Perform feature scaling or imputation after splitting the data.\n",
        "\n",
        "**3.) Time-Series Data:** Avoid using future data to predict past outcomes."
      ],
      "metadata": {
        "id": "1mMs5A34RWqw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agkxCqw5RXOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
      ],
      "metadata": {
        "id": "O0EnrrCYRF48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. **Definition:**\n",
        "\n",
        "A confusion matrix summarizes the performance of a classification model by comparing predicted vs. actual values.\n",
        "\n",
        "**Structure:**\n",
        "\n",
        "matrix[Actual Positive][Predicted Positive] = TP\n",
        "\n",
        "matrix[Actual Positive][Predicted Negative] = FN\n",
        "\n",
        "matrix[Actual Negative][Predicted Positive] = FP\n",
        "\n",
        "matrix[Actual Negative][Predicted Negative] = TN\n",
        "\n",
        "where,\n",
        "\n",
        "TP = True Positive\n",
        "\n",
        "FN = False Negative\n",
        "\n",
        "FP = False Positive\n",
        "\n",
        "TN = True Negative\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "It helps evaluate metrics like accuracy, precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "3F8PpeU1RYga"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7FW7E8CRZFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
      ],
      "metadata": {
        "id": "4b0ypbXDRH7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. **1.) Precision:**\n",
        "\n",
        "Definition: Proportion of true positive predictions out of all positive predictions.\n",
        "\n",
        "Formula:\n",
        "``` Precision = TP / (TP + FP) ```\n",
        "\n",
        "Focus: Reducing false positives.\n",
        "\n",
        "Example: Useful in spam detection.\n",
        "\n",
        "**2.) Recall:**\n",
        "\n",
        "Definition: Proportion of true positive predictions out of all actual positives.\n",
        "\n",
        "Formula:\n",
        "``` Recall = TP / (TP + FN) ```\n",
        "\n",
        "Focus: Reducing false negatives.\n",
        "\n",
        "Example: Useful in medical diagnoses."
      ],
      "metadata": {
        "id": "fmJOxsSmRZmw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAVxrCyGRaHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
      ],
      "metadata": {
        "id": "IeRVxyJuRKAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. **False Positives (FP):**\n",
        "\n",
        "Model predicts positive when actual is negative.\n",
        "\n",
        "Example: Predicting spam for non-spam emails.\n",
        "\n",
        "\n",
        "**False Negatives (FN):**\n",
        "\n",
        "Model predicts negative when actual is positive.\n",
        "\n",
        "Example: Failing to detect a disease in a patient.\n",
        "\n",
        "\n",
        "**Analyze FP and FN to understand where the model struggles (e.g., precision vs. recall trade-offs).**"
      ],
      "metadata": {
        "id": "bLiduqbVRbsW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFY-zlJCRcPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
        "calculated?"
      ],
      "metadata": {
        "id": "od4WjvqeRL_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. Some common metrics that can be derived from a confusion matrix are:\n",
        "\n",
        "1.) Accuracy\n",
        "```Accuracy = (TP + TN) / (TP + TN + FP + FN)```\n",
        "\n",
        "2.) Precision\n",
        "```Precision = TP/ (TP + FP)```\n",
        "\n",
        "3.) Recall(Sensititvity)\n",
        "``` Recall = TP / (TP + FN)```\n",
        "\n",
        "4.) Specificity\n",
        "``` Specificity = TN / (TN + FP)```\n",
        "\n",
        "5.) F1- score\n",
        "``` F1-score = 2 * [(Precision * Recall) / (Precision + Recall)] ```"
      ],
      "metadata": {
        "id": "mcgC-T4uRc5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0NOYVuVRdb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
      ],
      "metadata": {
        "id": "GcCKiLRCROIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9. Accuracy measures the proportion of correct predictions (TP + TN) but can be misleading in imbalanced datasets.\n",
        "\n",
        "Example: For a dataset with 95% negatives, predicting all negatives yields 95% accuracy but fails to identify positives."
      ],
      "metadata": {
        "id": "OSYu48jwRejH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ee1Ck2aRfDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
        "model?"
      ],
      "metadata": {
        "id": "EkUkMrAuRPzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A10. **Class Imbalance:**\n",
        "\n",
        "High FN or FP for a minority class indicates poor handling of class imbalance.\n",
        "\n",
        "\n",
        "**Bias Detection:**\n",
        "\n",
        "Disparities in FN or FP across demographic groups can indicate bias.\n",
        "\n",
        "\n",
        "**Corrective Actions:**\n",
        "\n",
        "Use resampling techniques or adjust class weights to address imbalance.\n",
        "\n",
        "Evaluate fairness metrics to ensure unbiased predictions.\n"
      ],
      "metadata": {
        "id": "Dd1tQ9Y0RfuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpwDEYOiRgRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}