{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC: Understanding Pooling and Padding in CNN\n"
      ],
      "metadata": {
        "id": "9zu4CgZMVEZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Describe the purpose and benefits of pooling in CNN.\n"
      ],
      "metadata": {
        "id": "SoNzwPe8VMs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Purpose: Pooling layers reduce the spatial dimensions (width and height) of the feature maps, preserving important information while reducing computation and overfitting.\n",
        "\n",
        "## Benefits:\n",
        "\n",
        "1. Dimensionality reduction: Speeds up training and inference.\n",
        "\n",
        "2. Translation invariance: Helps the model become more robust to small shifts and distortions.\n",
        "\n",
        "3. Reduces overfitting: By summarizing feature presence rather than location.\n",
        "\n"
      ],
      "metadata": {
        "id": "IUJOkgZXYkdt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUZ3mVG9WS8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the difference between min pooling and max pooling."
      ],
      "metadata": {
        "id": "sH2WPMMqWTSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Difference between Max and Min Pooling are:         \n",
        "\n",
        "## **Max Pooling**\n",
        "\n",
        "1. Selects the **maximum** value in each patch.\n",
        "\n",
        "2. Focuses on **strongest features**.\n",
        "\n",
        "3. Commonly used in practice.\n",
        "\n",
        "## **Min Pooling**      \n",
        "\n",
        "1. Selects the **minimum** value in each patch.                 |\n",
        "\n",
        "2. Retains **least activated features**.\n",
        "\n",
        "3. Rarely used, typically for experimentation or special cases.\n"
      ],
      "metadata": {
        "id": "zk5pkUebY23L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBZ1jq0dWStZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Discuss the concept of padding in CNN and its significance."
      ],
      "metadata": {
        "id": "WiTpuGXHWO_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. **Padding:** It involves adding extra pixels (usually zeros) around the input image.\n",
        "\n",
        "## Significance:\n",
        "\n",
        "1. Controls output size: Prevents shrinking of feature maps after each convolution.\n",
        "\n",
        "2. Preserves edge information: Ensures features at the borders are not ignored.\n",
        "\n",
        "3. Allows deeper networks: Maintains spatial dimensions, enabling more layers without losing too much resolution."
      ],
      "metadata": {
        "id": "aCWALvryZttb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q95YEr7_MwZ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
      ],
      "metadata": {
        "id": "qv9eLeEWWMu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4.\n",
        "\n",
        "| **Zero Padding (\"same\")**   | **Valid Padding**                          |\n",
        "| --------------------------- | ------------------------------------------ |\n",
        "| Pads input with zeros.      | No padding applied.                        |\n",
        "| Output size ≈ Input size.   | Output size shrinks with each convolution. |\n",
        "| Preserves spatial features. | Might lose edge information.               |\n",
        "| Useful for deep networks.   | Used when reducing size is intentional.    |\n",
        "\n",
        "\n",
        "Summary:\n",
        "\n",
        "Use zero-padding when you want to maintain spatial dimensions.\n",
        "\n",
        "Use valid-padding when you want to reduce dimensions intentionally.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0Yct8KQZ4Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jknwIvmdWMNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv5CcUtyVvOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC: Exploring LeNet"
      ],
      "metadata": {
        "id": "Krds13GKVs2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Provide a brief overview of LeNet-5 architecture."
      ],
      "metadata": {
        "id": "gteaQ4fnVctB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun in 1998 for handwritten digit recognition (MNIST dataset).\n",
        "\n",
        "It introduced key concepts like convolutions, pooling, and fully connected layers, forming the backbone of modern CNNs."
      ],
      "metadata": {
        "id": "hosqmMrTa6Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOBBydZfWhho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Describe the key components of LeNet-5 and their respective purposes."
      ],
      "metadata": {
        "id": "cuLMdqggWdkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2.\n",
        "\n",
        "| **Layer**       | **Type**                   | **Purpose**                          |\n",
        "| --------------- | -------------------------- | ------------------------------------ |\n",
        "| **Input Layer** | 32×32 grayscale image      | Accepts input image (e.g., digit)    |\n",
        "| **C1**          | Convolutional (6\\@5x5)     | Extracts local features              |\n",
        "| **S2**          | Subsampling (2x2 avg)      | Downsamples to reduce dimensionality |\n",
        "| **C3**          | Convolutional (16\\@5x5)    | Extracts deeper patterns             |\n",
        "| **S4**          | Subsampling (2x2 avg)      | Further downsampling                 |\n",
        "| **C5**          | Fully connected conv (120) | Transition to dense features         |\n",
        "| **F6**          | Fully connected (84)       | High-level feature combination       |\n",
        "| **Output**      | Fully connected (10)       | Predicts digit class (0-9)           |\n",
        "\n",
        "\n",
        "Note: Activation function used is tanh in the original architecture."
      ],
      "metadata": {
        "id": "3OGH_ITWa-or"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "du768BqgWdHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
      ],
      "metadata": {
        "id": "HOHpumzMWay0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. Advantages and Limitations in Image Classification are -\n",
        "\n",
        "## Advantages:\n",
        "\n",
        "1. **Lightweight:** Fewer parameters compared to modern networks.\n",
        "\n",
        "2. **Interpretable:** Easy to understand and visualize.\n",
        "\n",
        "3. **Efficient:** Fast on small datasets (e.g., MNIST).\n",
        "\n",
        "\n",
        "## Limitations:\n",
        "\n",
        "1. **Poor scalability:** Not suitable for large or complex datasets (e.g., ImageNet).\n",
        "\n",
        "2. **Shallow:** Lacks deep feature extraction compared to modern CNNs.\n",
        "\n",
        "3. **Limited to grayscale:** Originally designed for 1-channel inputs."
      ],
      "metadata": {
        "id": "d6zRlo8BbGwG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDbIbE0EWaP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
      ],
      "metadata": {
        "id": "UhGI48Y4WXqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A4. Implementing LeNet-5 using Tensorflow.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_lenet5():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(32, 32, 1)))  # Input Layer\n",
        "\n",
        "    model.add(layers.Conv2D(6, kernel_size=(5, 5), activation='tanh'))  # C1\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2)))  # S2\n",
        "\n",
        "    model.add(layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'))  # C3\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2)))  # S4\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(120, activation='tanh'))  # C5\n",
        "    model.add(layers.Dense(84, activation='tanh'))   # F6\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # Output Layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile and view the model\n",
        "lenet_model = build_lenet5()\n",
        "lenet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lenet_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "rTrcK2w6beBR",
        "outputId": "77513b54-40d7-4f0a-d0eb-0227fae14700"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m48,120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8J48nbwQzhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLrPCs4sWEQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC: Analyzing AlexNet"
      ],
      "metadata": {
        "id": "zEwPwiopWBYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Present an overview of the AlexNet architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QvlGOd9KVxtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. AlexNet, developed by Alex Krizhevsky et al. in 2012, revolutionized deep learning by winning the ImageNet LSVRC-2012 competition by a large margin.\n",
        "\n",
        "It demonstrated the power of deep convolutional neural networks on large-scale image classification.\n",
        "\n",
        "Input: 227×227×3 image\n",
        "Output: 1000 class probabilities (ImageNet classes)\n"
      ],
      "metadata": {
        "id": "epTAp2MsbnqC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuw6gaElQzki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough\n",
        "performance."
      ],
      "metadata": {
        "id": "SeQ7gi1DWppR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Architectural Innovations of AlexNet\n",
        "\n",
        "## Key Innovations:\n",
        "\n",
        "1. **ReLU activation:** Introduced ReLU instead of tanh/sigmoid for faster training.\n",
        "\n",
        "2. **GPU parallelism:** Split network across two GPUs to speed up training.\n",
        "\n",
        "3. **Dropout:** Used to prevent overfitting in fully connected layers.\n",
        "\n",
        "4. **Data Augmentation:** Random cropping, flipping, and color jittering.\n",
        "\n",
        "5. **Local Response Normalization (LRN):** For contrast enhancement (now rarely used)."
      ],
      "metadata": {
        "id": "CA5wiNf9bxPv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Or931uvaQzqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
      ],
      "metadata": {
        "id": "RsbMATMzWnYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3.\n",
        "\n",
        "| **Layer Type**             | **Purpose**                                                         |\n",
        "| -------------------------- | ------------------------------------------------------------------- |\n",
        "| **Convolutional Layers**   | Feature extraction using filters (capture spatial hierarchies).     |\n",
        "| **Pooling Layers**         | Downsample feature maps and make representations more invariant.    |\n",
        "| **Fully Connected Layers** | Combine high-level features to make final classification decisions. |\n",
        "| **ReLU Activation**        | Speeds up convergence and adds non-linearity.                       |\n",
        "| **Dropout**                | Prevents overfitting by randomly deactivating neurons.              |\n"
      ],
      "metadata": {
        "id": "o_Z-1pCncDJo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QikatqtQztm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
      ],
      "metadata": {
        "id": "Y08NQdOlWkja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. AlexNet Implementation in TensorFlow/Keras\n",
        "\n",
        "We'll use CIFAR-10 (10 classes, 32×32 images) for simplicity."
      ],
      "metadata": {
        "id": "mC0gQ9fMcNV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "x_train = tf.image.resize(x_train / 255.0, (227, 227))\n",
        "x_test = tf.image.resize(x_test / 255.0, (227, 227))\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# AlexNet Model\n",
        "def build_alexnet():\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=(227, 227, 3)))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), padding=\"same\", activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "    model.add(layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(layers.Conv2D(384, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # For CIFAR-10\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile and train\n",
        "alexnet = build_alexnet()\n",
        "alexnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "alexnet.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "vWXPorLiQzwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Insights\n",
        "\n",
        "1. AlexNet marked the deep learning boom in computer vision.\n",
        "\n",
        "2. Its modular structure inspired many architectures like VGG, ZFNet, and even early ResNets.\n",
        "\n",
        "3. You can further improve performance by training on ImageNet-sized datasets or using pretrained weights."
      ],
      "metadata": {
        "id": "ZUgBI6rpcasU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXOIdf4ZWrWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Md4rilGWr-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}