{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module72 KNN Assignment1"
      ],
      "metadata": {
        "id": "b3diZNUYHA4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?"
      ],
      "metadata": {
        "id": "V-aFg4LnCyIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression tasks.\n",
        "\n",
        "It works by finding the 'k' closest data points (neighbors) to a given query point and making predictions based on the majority class (for classification) or the average value (for regression) of these neighbors."
      ],
      "metadata": {
        "id": "U69l7DlwC0X6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "one7XQ2QC3GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "qLd_QKDQC5h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Choosing the value of K is crucial for the performance of the KNN algorithm:\n",
        "\n",
        "A.) **Low K (e.g., K=1):** Makes the model sensitive to noise, leading to overfitting.\n",
        "\n",
        "B.) **High K:** Results in oversmoothing, which may miss important local patterns and underfit.\n",
        "\n",
        "C.) **Optimal K:** Can be chosen using techniques like cross-validation or the \"elbow method,\" where K is selected to minimize error on a validation set.\n"
      ],
      "metadata": {
        "id": "jOrNz1gWC6XD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0U1Iw02DK14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between KNN classifier and KNN regressor?"
      ],
      "metadata": {
        "id": "JfFxTK3mDNFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. The difference between KNN classifier and KNN regressor are on the following aspects are:-\n",
        "\n",
        "# 1.)**KNN Classifier:**\n",
        "\n",
        "**Output** - Predicts the most common class label among neighbors.\n",
        "\n",
        "**Use case** - Classification tasks (categorical output).\n",
        "\n",
        "**Decision Boundary** - Results in discrete class boundaries.\n",
        "\n",
        "\n",
        "\n",
        "# 2.) **KNN Regressor:**\n",
        "\n",
        "**Output** - Predicts the average of the neighbors' values.\n",
        "\n",
        "**Use case** - Regression tasks (continuous output).\n",
        "\n",
        "**Decision Boundary** - Outputs a smooth regression curve.\n",
        "\n"
      ],
      "metadata": {
        "id": "q1lrtWj6DOhi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbpAbnu7EJxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How do you measure the performance of KNN?"
      ],
      "metadata": {
        "id": "T9GugiE_EMQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. We can measure the perforamnce of KNN :\n",
        "\n",
        "**For Classification:** Use metrics like accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "**For Regression:** Use metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared.\n",
        "Performance can also be validated using cross-validation or a train-test split."
      ],
      "metadata": {
        "id": "iXakVAeYEODO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdsXBZ3MEbWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the curse of dimensionality in KNN?"
      ],
      "metadata": {
        "id": "2e7ROzGmEdSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. The curse of dimensionality refers to the phenomenon where the distance between data points becomes less meaningful as the number of dimensions (features) increases. In KNN:\n",
        "\n",
        "1.) High-dimensional data makes all points appear equidistant.\n",
        "\n",
        "2.) This reduces the effectiveness of the algorithm, leading to poor predictions.\n",
        "\n",
        "3.) Solution: Use dimensionality reduction techniques like PCA or feature selection."
      ],
      "metadata": {
        "id": "Z8ofHzK2EejJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vB6pWshEla4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do you handle missing values in KNN?"
      ],
      "metadata": {
        "id": "tmYober8EnyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. We can handle missing values in KNN by folowing:\n",
        "\n",
        "1.) **Imputation Before Applying KNN:**\n",
        "\n",
        "a.) For numerical features: Use mean, median, or mode.\n",
        "\n",
        "b.) For categorical features: Use the most frequent category.\n",
        "\n",
        "\n",
        "\n",
        "2.) **KNN-Based Imputation:**\n",
        "\n",
        "a.) Predict missing values using KNN itself by treating the feature with missing values as the target variable and using the other features to find nearest neighbors."
      ],
      "metadata": {
        "id": "wPWhHshkEomj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYhtIE_ZE4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
      ],
      "metadata": {
        "id": "3rXyzEpfE9TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. The performance of the KNN classifier and regressor-\n",
        "\n",
        "**KNN Classifier:** Performs well for discrete/categorical targets. It works best for low-dimensional, evenly distributed data and distinct class boundaries.\n",
        "\n",
        "**KNN Regressor:** Suitable for continuous outputs. It works well when the relationship between features and the target variable is smooth.\n",
        "\n",
        "**Comparison:** Neither is inherently better; the choice depends on whether the problem is classification or regression."
      ],
      "metadata": {
        "id": "Sb1AeD4xE_Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5mTBOBuHFLGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
      ],
      "metadata": {
        "id": "-ZIPABfAFStc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8.\n",
        "\n",
        "# Strengths:\n",
        "\n",
        "Simple and intuitive.\n",
        "\n",
        "No assumptions about data distribution.\n",
        "\n",
        "Effective for small datasets and problems with distinct patterns.\n",
        "\n",
        "\n",
        "# Weaknesses:\n",
        "\n",
        "Computationally expensive as it requires storing and searching the entire dataset for each query.\n",
        "\n",
        "Sensitive to irrelevant features and feature scaling.\n",
        "\n",
        "Poor performance in high-dimensional spaces (curse of dimensionality).\n",
        "\n",
        "Can be affected by imbalanced datasets.\n",
        "\n",
        "\n",
        "# How to Address Weaknesses:\n",
        "\n",
        "Use dimensionality reduction (e.g., PCA).\n",
        "\n",
        "Apply feature scaling (e.g., normalization or standardization).\n",
        "\n",
        "Optimize K using cross-validation.\n",
        "\n",
        "Use approximate nearest neighbor methods for faster computation."
      ],
      "metadata": {
        "id": "onql0udNFUrq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecizjtY_FkzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
      ],
      "metadata": {
        "id": "jN8zQ4EnFn5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9. The difference between Euclidean distance and Manhattan distance in KNN are -\n",
        "\n",
        "# Euclidean distance\n",
        "\n",
        "**Formula:** ``` sq_root(Summation of (xi-yi)^2)```\n",
        "\n",
        "**Nature:** Measures straight-line (direct) distance.\n",
        "\n",
        "**Use case:** Works better when features are continuous and smooth.\n",
        "\n",
        "**Behaviour:** Sensitive to large differences in feature values.\n",
        "\n",
        "# Manhattan Distance\n",
        "\n",
        "**Formula:** ```[(x2-x1) + (y2-y1)] ```\n",
        "\n",
        "**Nature:** Measures grid-like (step-wise) distance.\n",
        "\n",
        "**Use case:** Works better when features are sparse or have grid-like patterns.\n",
        "\n",
        "**Behaviour:** Less sensitive to large differences in feature values."
      ],
      "metadata": {
        "id": "IUcqXBjUFpPK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGDrcsoSGtkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "pzkAmo1oGzH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A10. Feature scaling ensures that all features contribute equally to the distance calculation. Since KNN uses distance metrics (like Euclidean or Manhattan):\n",
        "\n",
        "1.) Features with larger ranges can dominate the distance calculation.\n",
        "\n",
        "2.) Scaling methods like standardization (z-score normalization) or min-max normalization are used to bring all features to a similar scale.\n",
        "\n",
        "3.) Without scaling, KNN can produce biased results."
      ],
      "metadata": {
        "id": "fJ9B94y3G0__"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HDzWqLb-G8bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKRJYzvGG_GD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}