{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
        "can they be mitigated?"
      ],
      "metadata": {
        "id": "FLqHFLWl_r0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1: A.)Overfitting: A model learns the training data too well, including noise and outliers, making it perform poorly on unseen data.\n",
        "\n",
        "Consequences: High training accuracy but poor test accuracy; fails to generalize to new data.\n",
        "\n",
        "Mitigation:\n",
        "\n",
        "1.) Use more training data.\n",
        "\n",
        "2.) Regularization techniques (e.g., L1/L2 regularization).\n",
        "\n",
        "3.) Prune complex models.\n",
        "\n",
        "4.) Use cross-validation to fine-tune hyperparameters.\n",
        "\n",
        "\n",
        "B.) Underfitting: A model is too simple to capture the underlying patterns in the data.\n",
        "\n",
        "Consequences: Poor performance on both training and test data; fails to capture relationships in the data.\n",
        "\n",
        "Mitigation:\n",
        "\n",
        "1.) Use more complex models.\n",
        "\n",
        "2.) Add more relevant features.\n",
        "\n",
        "3.) Train for more epochs (for deep learning).\n",
        "\n",
        "4.) Reduce regularization.\n"
      ],
      "metadata": {
        "id": "ShibggEcBM-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: How can we reduce overfitting? Explain in brief.\n",
        "\n"
      ],
      "metadata": {
        "id": "59qRQJte_r3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2: We can reduce overfitting by:-\n",
        "\n",
        "1.) Regularization: Add penalties to the loss function for large weights (L1 or L2 regularization).\n",
        "\n",
        "2.) Cross-Validation: Split data into training and validation sets to tune hyperparameters.\n",
        "\n",
        "3.) Pruning Models: Simplify the model by reducing the number of parameters or nodes in trees/networks.\n",
        "\n",
        "4.) Early Stopping: Stop training when validation performance stops improving.\n",
        "\n",
        "5.) Data Augmentation: Increase the diversity of training data (e.g., for images, rotate, flip, or crop).\n",
        "\n",
        "6.) Dropout: Randomly disable neurons during training in neural networks to prevent over-reliance on specific neurons.\n",
        "\n",
        "7.) Use More Data: Larger datasets reduce the chance of overfitting."
      ],
      "metadata": {
        "id": "Q5KSTNiDBoFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
      ],
      "metadata": {
        "id": "12Ru8ROr_r6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3: Underfitting: Occurs when a model cannot capture the underlying patterns in the data due to its simplicity.\n",
        "\n",
        "Scenarios Where Underfitting Occurs :-\n",
        "\n",
        "1.) Using linear models for non-linear data (e.g., fitting a straight line to a curved relationship).\n",
        "\n",
        "2.) Insufficient training (e.g., stopping training too early).\n",
        "\n",
        "3.) Model regularization is too strong.\n",
        "\n",
        "4.) Features used are insufficient or irrelevant."
      ],
      "metadata": {
        "id": "ZvEF0NSuCROb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?"
      ],
      "metadata": {
        "id": "uaIMAaCa_r_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4: Bias: Error due to overly simplistic assumptions. High bias leads to underfitting.\n",
        "\n",
        "Variance: Error due to sensitivity to small fluctuations in training data. High variance leads to overfitting.\n",
        "\n",
        "\n",
        "Tradeoff:\n",
        "\n",
        "1.) Low bias typically increases variance, and low variance typically increases bias.\n",
        "2.) The goal is to find a balance between bias and variance for optimal generalization.\n",
        "\n",
        "\n",
        "Impact on Model Performance:\n",
        "\n",
        "1.) High Bias: Poor accuracy on training and test sets.\n",
        "2.) High Variance: Good training accuracy but poor test accuracy."
      ],
      "metadata": {
        "id": "e2CF6lK5CZeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?"
      ],
      "metadata": {
        "id": "1EUMJ6s6_sFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5: Discuss some common methods for detecting overfitting and underfitting in machine learning models are:-\n",
        "\n",
        "1.) Check Performance on Train vs. Test Data:\n",
        "\n",
        "Overfitting: High train accuracy, low test accuracy.\n",
        "Underfitting: Low accuracy on both train and test sets.\n",
        "\n",
        "\n",
        "2.) Learning Curves:\n",
        "\n",
        "Plot train and validation loss/accuracy over epochs.\n",
        "Diverging curves: Overfitting.\n",
        "Parallel curves with high error: Underfitting.\n",
        "\n",
        "\n",
        "3.) Cross-Validation: Use a validation set to monitor performance during training."
      ],
      "metadata": {
        "id": "9FSyJSeKDD-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?"
      ],
      "metadata": {
        "id": "MfTJJ3XL_sIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6: These are the aspects for differentiation:-\n",
        "\n",
        "1.) Definition:\n",
        "\n",
        "Bias - \tError from simplistic assumptions\n",
        "\n",
        "Variance - Error from model sensitivity to data\n",
        "\n",
        "2.) Model Type:\n",
        "\n",
        "Bias - High-bias models (underfit)\n",
        "\n",
        "Variance - High-variance models (overfit)\n",
        "\n",
        "3.) Impact:\n",
        "\n",
        "Bias - Poor training and test accuracy\n",
        "\n",
        "Variance - Poor test accuracy; good train accuracy\n",
        "\n",
        "4.) Examples:\n",
        "\n",
        "Bias - Linear regression for non-linear data\n",
        "\n",
        "Variance - Decision trees without pruning"
      ],
      "metadata": {
        "id": "7Olh1v5hD0RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work."
      ],
      "metadata": {
        "id": "nfLLtcmx_sK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7: Regularization: A technique to penalize large weights in a model, reducing its complexity and preventing overfitting.\n",
        "\n",
        "Common Regularization Techniques:\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "Adds the absolute value of weights to the loss function ( ùúÜ ‚àë ‚à£ùë§| ) .\n",
        "Results in sparse models by setting some weights to 0 (feature selection).\n",
        "L2 Regularization (Ridge):\n",
        "\n",
        "Adds the squared value of weights to the loss function ( ùúÜ ‚àë ùë§ 2  ).\n",
        "Distributes the weight penalty across all features.\n",
        "Elastic Net: Combines L1 and L2 regularization.\n",
        "\n",
        "Dropout: Randomly disables neurons during training in neural networks to reduce over-reliance on specific pathways.\n",
        "\n",
        "Early Stopping: Stops training when validation loss stops improving."
      ],
      "metadata": {
        "id": "Qf2ByJ-5FEQK"
      }
    }
  ]
}