{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 62 Regression2 Ridge Assignment"
      ],
      "metadata": {
        "id": "bquYwHtVuxU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
      ],
      "metadata": {
        "id": "ysp8j0D3uS3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Ridge Regression:\n",
        "\n",
        "A type of linear regression that adds an L2 -norm penalty to the loss function, which helps reduce overfitting by shrinking coefficients.\n",
        "\n",
        "**Objective Function:**\n",
        "\n",
        "`L= MSE + λ *(j=1 to p ∑ ∣βj∣^2 ) `\n",
        "\n",
        "where, λ: Regularization parameter that controls the penalty strength.\n",
        "\n",
        "# Difference from OLS:\n",
        "\n",
        "1.) OLS minimizes only the mean squared error (MSE), which may lead to large coefficients in the presence of multicollinearity or overfitting.\n",
        "\n",
        "2.) Ridge includes a penalty term, reducing the magnitude of coefficients, especially for less relevant features.\n",
        "\n"
      ],
      "metadata": {
        "id": "2vkI1XGCuS6S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0gC0xVouVFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the assumptions of Ridge Regression?"
      ],
      "metadata": {
        "id": "hPw4QzG4uS8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Ridge Regression shares assumptions with OLS regression, with the addition of the penalty term:\n",
        "\n",
        "**1.) Linearity:** The relationship between predictors and the target is linear.\n",
        "\n",
        "**2.) No Perfect Multicollinearity:** Ridge reduces multicollinearity but doesn’t handle perfect correlation.\n",
        "\n",
        "**3.) Homoscedasticity:** Constant variance of residuals.\n",
        "\n",
        "**4.) Normality of Errors:** Residuals should follow a normal distribution.\n",
        "\n",
        "**5.) Independence of Errors:** Residuals are uncorrelated with one another."
      ],
      "metadata": {
        "id": "8PdlY43muS-x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXH3p1WLuV1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
      ],
      "metadata": {
        "id": "10wC_vfVuTBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3.\n",
        "\n",
        "**1.) Cross-Validation:**\n",
        "\n",
        "Use k-fold cross-validation to evaluate model performance for different λ values.\n",
        "\n",
        "Select the λ that minimizes the validation error.\n",
        "\n",
        "\n",
        "**2.) Grid Search:**\n",
        "\n",
        "Test a range of λ values using GridSearchCV.\n",
        "\n",
        "\n",
        "**3.) Built-In Methods:**\n",
        "\n",
        "Use RidgeCV in Python for automated cross-validation and selection of λ."
      ],
      "metadata": {
        "id": "KIkLkaxDuTEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset: Create a synthetic regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas= np.logspace(-4, 4, 50), cv=5)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "print(f\"Optimal Lambda: {ridge_cv.alpha_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mScf6v6wuWnQ",
        "outputId": "bdca01b0-5ab6-4cf4-fd68-c7268787b113"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Lambda: 0.0029470517025518097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
      ],
      "metadata": {
        "id": "Ye0UMzIsuTHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. **No Direct Feature Selection:**\n",
        "\n",
        "Ridge Regression shrinks coefficients but doesn’t reduce any to exactly zero (unlike Lasso Regression).\n",
        "\n",
        "All features remain in the model with reduced influence for less relevant ones.\n",
        "\n",
        "\n",
        "**Alternative Approach:**\n",
        "\n",
        "Use Ridge in combination with other techniques like Recursive Feature Elimination (RFE) for feature selection."
      ],
      "metadata": {
        "id": "u2wAUq5AuTJy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6IUWOkpuXpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
      ],
      "metadata": {
        "id": "bjSW2g9muTMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. **Effectiveness:**\n",
        "\n",
        "Ridge Regression reduces the impact of multicollinearity by shrinking correlated coefficients towards each other.\n",
        "\n",
        "This prevents the model from being overly influenced by noise or redundant features.\n",
        "\n",
        "\n",
        "**Why It Works:**\n",
        "\n",
        "The penalty term stabilizes coefficient estimates, making them less sensitive to slight variations in the data."
      ],
      "metadata": {
        "id": "V2KGI1mYuTPG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spEZtgANuYeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
      ],
      "metadata": {
        "id": "dfrvEpcBuTSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. Yes, with Preprocessing:\n",
        "\n",
        "Continuous Variables: Directly used in the model.\n",
        "\n",
        "Categorical Variables: Must be encoded into numerical form (e.g., one-hot encoding or label encoding).\n",
        "\n",
        "```\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "ridge_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', Ridge(alpha=1.0))\n",
        "])\n",
        "\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7ekcQpRUuuLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do you interpret the coefficients of Ridge Regression?"
      ],
      "metadata": {
        "id": "fB9u-Hz6uTYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Ridge coefficients represent the effect of a one-unit change in the predictor variable on the target variable, assuming other predictors remain constant.\n",
        "\n",
        "Shrinking of coefficients makes interpretation less straightforward compared to OLS.\n",
        "\n",
        "Larger λ values lead to smaller coefficients, reflecting reduced importance of certain features.\n"
      ],
      "metadata": {
        "id": "A9x4EYRius9_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UOD4IwLutir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
      ],
      "metadata": {
        "id": "j436XGNxuTb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. Yes, Ridge Regression can be used for time-series analysis with appropriate modifications:\n",
        "\n",
        "**1.) Feature Engineering:**\n",
        "Create lagged variables or rolling statistics to account for temporal dependencies.\n",
        "\n",
        "**2.) Train-Test Split:**\n",
        "Use time-based splitting to ensure no future data is used in training.\n",
        "\n",
        "**3.) Regularization Benefits:**\n",
        "Helps handle multicollinearity that often arises from overlapping time-related features (e.g., multiple lags).\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Example data\n",
        "data['Lag_1'] = data['Value'].shift(1)\n",
        "data['Lag_2'] = data['Value'].shift(2)\n",
        "data = data.dropna()\n",
        "\n",
        "X = data[['Lag_1', 'Lag_2']]\n",
        "y = data['Value']\n",
        "\n",
        "# Time-based cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    ridge.fit(X_train, y_train)\n",
        "    print(f\"Score: {ridge.score(X_test, y_test)}\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "5OdeLQveusHD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lTkJaksxYfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}