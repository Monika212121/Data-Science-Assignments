{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module66 Logistic Regression Assignment1"
      ],
      "metadata": {
        "id": "TYNt4AFUGulh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
        "a scenario where logistic regression would be more appropriate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKBPEZweGvDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. The difference between Linear Regression and Logistic Regression are:-\n",
        "\n",
        "**Linear Regression:** Predicts a continuous numeric value (e.g., house prices, stock values). The output is directly modeled as:\n",
        "\n",
        "``` y = β0 + β1*x1 + β2*x2 + ..... + βn*xn  ```\n",
        "\n",
        "**Logistic Regression:** Predicts probabilities for binary or categorical outcomes (e.g., spam vs. not spam, pass vs. fail). Uses a sigmoid function to transform the output into probabilities:\n",
        "\n",
        "``` P(y=1) = { 1 / [1 + e^-(β0 +β1*x1 +β2*x2 + …)] } ```\n",
        "\n",
        "Scenario for Logistic Regression:\n",
        "\n",
        "Predicting whether an email is spam (1) or not spam (0) based on features like word frequency."
      ],
      "metadata": {
        "id": "KXlWcnFOG9kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RB9e00u1HWWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the cost function used in logistic regression, and how is it optimized?"
      ],
      "metadata": {
        "id": "4lgoJ29pG9Yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Cost Function: Logistic regression uses the log-loss function (cross-entropy loss):\n",
        "\n",
        "``` J(θ)= (- 1/n) * { ∑(from i=1 to n) [yi*log(hθ(Xi))+(1-yi)*log(1-hθ(Xi))]}```\n",
        "\n",
        "where,\n",
        "\n",
        "hθ(Xi) =  Predicted probability for the i-th sample.\n",
        "\n",
        "yi = Actual label (0 or 1).\n",
        "\n",
        "\n",
        "**Optimization:**\n",
        "\n",
        "Typically optimized using gradient descent or stochastic gradient descent (SGD).\n",
        "\n",
        "Gradients of the loss function are calculated to update weights iteratively."
      ],
      "metadata": {
        "id": "xfOAQDegHXDr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u99WNdELHXoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
      ],
      "metadata": {
        "id": "wD9iFbGMG9J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. **Concept:** Regularization adds a penalty to the cost function to constrain the magnitude of coefficients and reduce overfitting.\n",
        "\n",
        "Types are following:\n",
        "\n",
        "**1.) L1 Regularization (Lasso):** Adds a penalty proportional to the absolute value of coefficients.\n",
        "\n",
        "``` J(θ) = Log-Loss + λ * ∑ |βj| ```\n",
        "\n",
        "Shrinks some coefficients to zero, performing feature selection.\n",
        "\n",
        "**2.) L2 Regularization (Ridge):** Adds a penalty proportional to the square of coefficients.\n",
        "\n",
        "``` J(θ) = Log-Loss + λ * ∑ βj^2 ```\n",
        "\n",
        "Shrinks coefficients but doesn’t eliminate them entirely.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6gWq0D-HYfY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVMao76vHZC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
        "model?"
      ],
      "metadata": {
        "id": "MQa5rlHnG86S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. **ROC Curve (Receiver Operating Characteristic):** Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at various classification thresholds.\n",
        "\n",
        "**Area Under the Curve (AUC):** AUC-ROC measures the model’s ability to distinguish between classes.\n",
        "A higher AUC (close to 1) indicates better model performance.\n",
        "\n",
        "\n",
        "**Usage:**\n",
        "\n",
        "1.) Evaluate how well the logistic regression model separates positive and negative classes.\n",
        "\n",
        "2.) Helps in choosing the optimal classification threshold.\n"
      ],
      "metadata": {
        "id": "FYPpG-2BHaQb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oohCrlP0HayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
        "techniques help improve the model's performance?"
      ],
      "metadata": {
        "id": "dYcjTE7hG8tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. Some common  techniques in Feature selection in Logistic Regression are:-\n",
        "\n",
        "**1.) Univariate Statistical Tests:**\n",
        "\n",
        "Use tests like chi-square, ANOVA, or mutual information to rank features.\n",
        "\n",
        "**2.) Recursive Feature Elimination (RFE):**\n",
        "\n",
        "Iteratively remove least important features based on model performance.\n",
        "\n",
        "**3.) Regularization:**\n",
        "\n",
        "Use L1 regularization to shrink less important coefficients to zero.\n",
        "\n",
        "**4.) Feature Importance:**\n",
        "\n",
        "Use coefficients or other metrics to identify and retain significant features.\n",
        "\n",
        "**5.) Domain Knowledge:**\n",
        "\n",
        "Manually select features based on their relevance to the problem.\n",
        "\n",
        "# Benefits:\n",
        "Reduces noise, improves interpretability, and prevents overfitting."
      ],
      "metadata": {
        "id": "IzvYtCAbHbd7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dd5CWB7LHb9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
      ],
      "metadata": {
        "id": "0ZNfRdusG5LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. Handling Imbalanced datasets in Logistic Regression are:\n",
        "\n",
        "**1.) Resampling:**\n",
        "\n",
        "Oversampling: Increase samples of the minority class (e.g., SMOTE).\n",
        "\n",
        "Undersampling: Reduce samples of the majority class.\n",
        "\n",
        "\n",
        "**2.) Class Weights:**\n",
        "\n",
        "Assign higher weights to the minority class in the loss function.\n",
        "\n",
        "\n",
        "**3.) Threshold Tuning:**\n",
        "\n",
        "Adjust the decision threshold based on the ROC curve.\n",
        "\n",
        "\n",
        "**4.) Ensemble Methods:**\n",
        "\n",
        "Use methods like Random Forest or Gradient Boosting, which handle imbalance better.\n",
        "\n",
        "\n",
        "**5.) Evaluation Metrics:**\n",
        "\n",
        "Use metrics like Precision-Recall AUC, F1-Score, or Confusion Matrix instead of accuracy."
      ],
      "metadata": {
        "id": "BO6NkxQCHcjh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StNtHwtpHdTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
        "regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
      ],
      "metadata": {
        "id": "zc15iSKZG39f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Some common issues and challenges arise when implementing Logistic Regression are:\n",
        "\n",
        "**1.) Multicollinearity:**\n",
        "\n",
        "Issue: High correlation between independent variables.\n",
        "\n",
        "Solution: Use Ridge Regularization to shrink correlated coefficients.\n",
        "\n",
        "Calculate Variance Inflation Factor (VIF) to detect multicollinearity and remove redundant features.\n",
        "\n",
        "\n",
        "**2.) Imbalanced Data:**\n",
        "\n",
        "Issue: Poor performance on minority classes.\n",
        "\n",
        "Solution: Apply class weights or resampling techniques.\n",
        "\n",
        "\n",
        "**3.) Non-Linearity:**\n",
        "\n",
        "Issue: Logistic regression assumes a linear relationship between features and log-odds.\n",
        "\n",
        "Solution: Use feature engineering or switch to non-linear models like Decision Trees.\n",
        "\n",
        "\n",
        "**4.) Outliers:**\n",
        "\n",
        "Issue: Outliers can disproportionately affect model performance.\n",
        "\n",
        "Solution: Detect and remove or transform outliers (e.g., scaling).\n",
        "\n",
        "\n",
        "**5.) Overfitting:**\n",
        "\n",
        "Issue: Model memorizes noise in the training data.\n",
        "\n",
        "Solution: Use regularization (L1/L2) or cross-validation."
      ],
      "metadata": {
        "id": "reafVQ_3Hdy6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QVmK_IQHeYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}