{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module74 Unsupervised Learning Clustering Ass 1"
      ],
      "metadata": {
        "id": "VptrjupNOeCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Different Types of Clustering Algorithms and Their Differences"
      ],
      "metadata": {
        "id": "qfmLnhE0MMlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Clustering algorithms group data points based on similarity. The main types are:\n",
        "\n",
        "### 1.) Partition-Based Clustering\n",
        "\n",
        "Example: K-Means, K-Medoids\n",
        "\n",
        "Approach: Divides data into K clusters by minimizing intra-cluster variance.\n",
        "\n",
        "Assumption: Data is spherical and equally sized clusters exist.\n",
        "\n",
        "\n",
        "### 2.) Hierarchical Clustering\n",
        "\n",
        "Example: Agglomerative, Divisive\n",
        "\n",
        "Approach: Builds a tree-like hierarchy of clusters using a bottom-up (Agglomerative) or top-down (Divisive) approach.\n",
        "\n",
        "Assumption: There is an inherent hierarchical structure in the data.\n",
        "\n",
        "\n",
        "### 3.) Density-Based Clustering\n",
        "\n",
        "Example: DBSCAN, OPTICS\n",
        "\n",
        "Approach: Groups points that are close to each other based on density and labels outliers.\n",
        "\n",
        "Assumption: Clusters are dense regions separated by low-density areas.\n",
        "\n",
        "\n",
        "###4.) Model-Based Clustering\n",
        "\n",
        "Example: Gaussian Mixture Models (GMMs)\n",
        "\n",
        "Approach: Assumes data is generated from a mixture of probabilistic distributions.\n",
        "\n",
        "Assumption: Clusters follow a statistical distribution (e.g., Gaussian).\n",
        "\n",
        "\n",
        "### 5.) Grid-Based Clustering\n",
        "\n",
        "Example: STING, CLIQUE\n",
        "\n",
        "Approach: Divides space into grids and clusters them.\n",
        "\n",
        "Assumption: Works well for large datasets but assumes uniform grid structures.\n"
      ],
      "metadata": {
        "id": "JUwJdFNUMPqw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xyb6kQ31MRBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is K-Means Clustering and How Does It Work?"
      ],
      "metadata": {
        "id": "rwCnoahPMrsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. K-Means is a partition-based clustering algorithm that divides data into K clusters by minimizing intra-cluster variance.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1.) Select K cluster centers (centroids) randomly.\n",
        "\n",
        "2.) Assign each point to the nearest centroid.\n",
        "\n",
        "3.) Compute new centroids as the mean of assigned points.\n",
        "\n",
        "4.) Repeat steps 2-3 until centroids don’t change or a stopping condition is met."
      ],
      "metadata": {
        "id": "IvIsqCFpMt7L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDHPxCl6MzsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Advantages and Limitations of K-Means Clustering"
      ],
      "metadata": {
        "id": "fSdYTXPCM2O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. Advantages and Limitations of K-Means Clustering are :-\n",
        "\n",
        "## Advantages:\n",
        "\n",
        "Fast and scalable for large datasets.\n",
        "\n",
        "Easy to interpret and implement.\n",
        "\n",
        "Works well for spherical and well-separated clusters.\n",
        "\n",
        "\n",
        "## Limitations:\n",
        "\n",
        "Requires predefined K (number of clusters).\n",
        "\n",
        "Sensitive to outliers, which can distort centroids.\n",
        "\n",
        "Struggles with non-spherical clusters or varying densities."
      ],
      "metadata": {
        "id": "2ggNHjl1M3Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-96YFoiNAsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How to Determine the Optimal Number of Clusters in K-Means?"
      ],
      "metadata": {
        "id": "CSX91tTaNH-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. Some common methods to determine the Optimal Number of Clusters in K-Means are :-\n",
        "\n",
        "### 1.) Elbow Method:\n",
        "\n",
        "Plot the Within-Cluster Sum of Squares (WCSS) vs. K.\n",
        "Choose the point where WCSS drops significantly (the “elbow”).\n",
        "\n",
        "\n",
        "### 2.) Silhouette Score:\n",
        "\n",
        "Measures how similar a point is to its cluster vs. others.\n",
        "\n",
        "Higher values indicate well-defined clusters.\n",
        "\n",
        "\n",
        "### 3.) Gap Statistic:\n",
        "\n",
        "Compares WCSS to randomly generated data distributions.\n"
      ],
      "metadata": {
        "id": "bTUqEIlNNJ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcZB9s6qNYyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Applications of K-Means Clustering in Real-World Scenarios"
      ],
      "metadata": {
        "id": "gJFRzCH-Nb2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. Some applications of K-Means Clustering in Real-World Scenarios are :-\n",
        "\n",
        "1.) **Customer Segmentation:** Grouping customers based on buying behavior.\n",
        "\n",
        "2.) **Image Compression:** Reducing colors in images by clustering pixel values.\n",
        "\n",
        "3.) **Anomaly Detection:** Identifying fraudulent transactions.\n",
        "\n",
        "4.) **Document Clustering:** Grouping similar documents for recommendation systems."
      ],
      "metadata": {
        "id": "hDL4Z_Y9NeD2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tj56TKm3Nuew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How to Interpret the Output of K-Means Clustering?"
      ],
      "metadata": {
        "id": "IkCScvpmNwvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. We can interpret the output of K-Mens clustering by following :-\n",
        "\n",
        "**Centroids** represent the “average” of each cluster.\n",
        "\n",
        "**Cluster assignments** indicate which data points belong together.\n",
        "\n",
        "Helps identify patterns, trends, or anomalies in data."
      ],
      "metadata": {
        "id": "QYoWXctJNzfy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4aqk84cNxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Challenges in Implementing K-Means and How to Address Them"
      ],
      "metadata": {
        "id": "VoDAIIfGOHp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. The challenges in Implementing K-Means and How to Address Them :\n",
        "\n",
        "1.) **Choosing K:** Use Elbow Method, Silhouette Score.\n",
        "\n",
        "2.) **Outliers:** Use K-Medoids or DBSCAN to handle noisy data.\n",
        "\n",
        "3.) **Unequal Cluster Sizes:** Use GMM instead of K-Means.\n",
        "\n",
        "4.) **Cluster Shape Issues:** If data isn’t spherical, consider hierarchical or density-based clustering.\n"
      ],
      "metadata": {
        "id": "LVZFZe_jOKHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0s0nTYnOa0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFMlvDyZOb_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}