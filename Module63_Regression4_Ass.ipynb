{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module63 Regression Lasso Assignment"
      ],
      "metadata": {
        "id": "H3CcUL5mzUHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
      ],
      "metadata": {
        "id": "Rw3-_EM5zL26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Lasso Regression (Least Absolute Shrinkage and Selection Operator):\n",
        "\n",
        "A linear regression technique that adds a penalty term proportional to the absolute values of the coefficients (‚à£ùõΩ‚à£) to the loss function. This leads to both shrinkage and feature selection.\n",
        "\n",
        "Objective Function:\n",
        "\n",
        "`L = MSE + ùúÜ * ‚àë(j=1 to p) ‚à£ùõΩùëó‚à£ `\n",
        "\n",
        "where, L = Loss function\n",
        "\n",
        "ùúÜ = Regularization parameter(controls penalty strength).\n",
        "\n",
        "‚à£ùõΩùëó‚à£ = Absolute value of the coefficients.\n",
        "\n",
        "\n",
        "\n",
        "**Key Difference:**\n",
        "\n",
        "Unlike standard linear regression, Lasso can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fWLVxTguzL53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUoCw59ayjk3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the main advantage of using Lasso Regression in feature selection?"
      ],
      "metadata": {
        "id": "KO7ENiXDzL8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. **Automatic Feature Selection:**\n",
        "\n",
        "1.) Lasso shrinks the coefficients of less important features to exactly zero, removing them from the model.\n",
        "\n",
        "2.) This simplifies the model, reduces overfitting, and makes it interpretable.\n",
        "\n",
        "**Advantage Over Ridge Regression:**\n",
        "\n",
        "While Ridge shrinks coefficients close to zero, it does not eliminate features entirely."
      ],
      "metadata": {
        "id": "QBGXD5fVzL_f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVc6d-KJzO4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you interpret the coefficients of a Lasso Regression model?"
      ],
      "metadata": {
        "id": "r9XATn8_zMCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. **Non-zero Coefficients:**\n",
        "\n",
        "Represent the features selected by the model. Larger coefficients indicate stronger influence on the target variable.\n",
        "\n",
        "**Zero Coefficients:**\n",
        "\n",
        "Features with zero coefficients are irrelevant to the model and are effectively excluded.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "In a house price prediction model, if the coefficient for `location` is non-zero and `age of house` is zero, the model considers `location` significant but ignores `age`."
      ],
      "metadata": {
        "id": "DOgp7hAKzME5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjIWYlyAzP1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
      ],
      "metadata": {
        "id": "gGZQN9kmzMHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4.\n",
        "\n",
        "#  Tuning Parameter - Œª(Regularization Strength):\n",
        "\n",
        "1.) Controls the magnitude of the penalty term.\n",
        "\n",
        "2.) Effects:\n",
        "\n",
        "**Small Œª:** Minimal penalty, behaves like linear regression.\n",
        "\n",
        "**Large Œª:** Strong penalty, shrinks more coefficients to zero, reducing overfitting but increasing bias.\n",
        "\n",
        "3.) Optimal Œª balances model complexity and performance."
      ],
      "metadata": {
        "id": "v-8TTqSrzMKl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Dg_5u0tzQkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
      ],
      "metadata": {
        "id": "RSJ4AWs-zMNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. Directly No: Lasso is inherently a linear regression technique.\n",
        "\n",
        "# Solution:\n",
        "\n",
        "Transform input features using polynomial features or other non-linear transformations, then apply Lasso.\n",
        "Example:\n",
        "\n",
        "Use polynomial regression with Lasso to capture non-linear relationships while performing feature selection."
      ],
      "metadata": {
        "id": "7u0znESCzMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDmOlCmnzRRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the difference between Ridge Regression and Lasso Regression?"
      ],
      "metadata": {
        "id": "YjzWXLCgzMSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. Difference between Ridge and Lasso Regression according to the following aspects :-\n",
        "\n",
        "**1.) Penalty**\n",
        "\n",
        "Ridge - Œª ‚àë (j= 1 to p) Œ≤j^2\n",
        "\n",
        "Lasso - Œª ‚àë (j= 1 to p) |Œ≤j|\n",
        "\n",
        "**2.) Featue Selection**\n",
        "\n",
        "Ridge - Does not perform feature selection; keeps all coefficients small.\n",
        "\n",
        "Lasso - Shrinks some coefficients to exactly zero, removing features.\n",
        "\n",
        "**3.) Usage**\n",
        "\n",
        "Ridge - For multicollinearity and when all features are important.\n",
        "\n",
        "Lasso - For sparse models and feature selection.\n",
        "\n",
        "**4.) Type of Regularization**\n",
        "\n",
        "Ridge - L2 Regularization\n",
        "\n",
        "Lasso - L1 Regularization"
      ],
      "metadata": {
        "id": "Kpe4LzZwzMV_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8MjxcD1zSm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
      ],
      "metadata": {
        "id": "b6QUiGRTzozD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Yes: Lasso can handle multicollinearity by:\n",
        "\n",
        "1.) Selecting one of the correlated features while shrinking others to zero.\n",
        "\n",
        "2.) Reducing redundancy and improving interpretability.\n",
        "\n",
        "**Limitation:** If two features are equally important, Lasso may arbitrarily select one."
      ],
      "metadata": {
        "id": "MWnZBIbhzqA7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPhSgQa2zrTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
      ],
      "metadata": {
        "id": "xMHqxHZ1zsRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. There are 2 methods to choose optimal regularization parameter :-\n",
        "\n",
        "**1.) Grid Search with Cross-Validation:**\n",
        "\n",
        "a.) Test a range of Œª values and evaluate the model's performance using k-fold cross-validation.\n",
        "\n",
        "b.) Select the Œª that minimizes validation error.\n",
        "\n",
        "**2.) Use Built-in Tools:**\n",
        "\n",
        "a.) In Python, LassoCV from scikit-learn automatically selects the optimal Œª."
      ],
      "metadata": {
        "id": "DV__rFcezr5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method - 1:  Grid Search with Cross-Validation\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset: Create a synthetic regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Lasso regression model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Define the grid of regularization parameters (alpha values)\n",
        "param_grid = {'alpha': np.logspace(-4, 0, 50)}  # Values ranging from 0.0001 to 1\n",
        "\n",
        "# Perform Grid Search with 5-Fold Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve the best regularization parameter\n",
        "optimal_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Optimal Regularization Parameter (Œª): {optimal_alpha}\")\n",
        "print(f\"Best Model: {best_model}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Test Set Mean Squared Error (MSE): {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dW61pl1Z4fF",
        "outputId": "faf992d4-bb97-4521-ad33-4eb655e44911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Optimal Regularization Parameter (Œª): 0.0016768329368110067\n",
            "Best Model: Lasso(alpha=0.0016768329368110067)\n",
            "Test Set Mean Squared Error (MSE): 0.0114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation  \n",
        "\n",
        "**1.) Dataset:**\n",
        "\n",
        "Make_regression is used to create a synthetic regression dataset. Replace it with your dataset if available.\n",
        "\n",
        "**2.) Grid of Regularization Parameters:**\n",
        "\n",
        "The `np.logspace(-4, 0, 50)` generates 50 values for Œ± between 10e-4 and 10e0. You can adjust the range or number of values as needed.\n",
        "\n",
        "**3.) GridSearchCV:**\n",
        "\n",
        "`cv=5` : 5-fold cross-validation is used to evaluate each Œ± value.\n",
        "\n",
        "`scoring ='neg_mean_squared_error'`: Negative MSE is used to measure performance (lower is better).\n",
        "\n",
        "**4.) Optimal Alpha:**\n",
        "\n",
        "The best regularization parameter (Œ±) is retrieved using\n",
        "`grid_search.best_params_['alpha'].`\n",
        "\n",
        "**5.) Model Evaluation:**\n",
        "\n",
        "The best Lasso model is evaluated on the test set using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "O-7ug5X2d6Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method- 2: Using Built in tools:-\n",
        "\n",
        "# Creating dataframe for implementing LassoCV\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "dataset = fetch_california_housing()\n",
        "df = pd.DataFrame(dataset.data, columns= dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "\n",
        "# Diving the dataset into dependent and independent features\n",
        "x = df.iloc[:, :-1]      # independent features\n",
        "y = df.iloc[:, -1]       # dependent feature\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Fit Lasso CV\n",
        "lasso_cv = LassoCV(cv=5).fit(x_train, y_train)\n",
        "\n",
        "# Finding Optimal lambda\n",
        "optimal_lambda = lasso_cv.alpha_\n",
        "print(f\"Optimal Regularization parameter(Œª) is: {optimal_lambda}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQBJh58zs2Z",
        "outputId": "5564d882-f02a-4918-cc15-0348d24dfbbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Regularization parameter(Œª) is: 0.034222561573497685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "233NxmQWYZyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}