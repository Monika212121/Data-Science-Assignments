{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module72 KNN Assignment2"
      ],
      "metadata": {
        "id": "_XwpCHHyILd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
      ],
      "metadata": {
        "id": "AkBuzvSkIORJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. **Euclidean Distance:** Measures the straight-line (direct) distance between two points. It emphasizes large differences in feature values and is sensitive to outliers.\n",
        "\n",
        "**Manhattan Distance:** Measures the grid-like (step-wise) distance between two points. It focuses on the sum of absolute differences and is less sensitive to outliers.\n",
        "\n",
        "# Impact on KNN Performance:\n",
        "\n",
        "**Euclidean Distance** is better suited for smooth, continuous data where relationships between features are geometric.\n",
        "\n",
        "**Manhattan Distance** works better with high-dimensional data or datasets with sparse features, as it mitigates the effect of individual feature variances.\n",
        "\n",
        "Choosing the wrong distance metric can lead to suboptimal predictions, especially if the metric does not align with the dataâ€™s structure."
      ],
      "metadata": {
        "id": "uI8rTjQLIN0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zwsSC1BIMJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
      ],
      "metadata": {
        "id": "z0A5SYE3IuG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2.\n",
        "\n",
        "# Challenges of k Selection:\n",
        "\n",
        "1.) **Small k (e.g., 1):** Sensitive to noise, leading to overfitting.\n",
        "\n",
        "2.) **Large k:** Tends to oversmooth the decision boundary, leading to underfitting.\n",
        "\n",
        "# Techniques to Choose Optimal k:\n",
        "\n",
        "1.) **Cross-Validation:** Split the data into training and validation sets, and evaluate the model's performance for various k values.\n",
        "\n",
        "2.) **Elbow Method:** Plot the error rate (or validation accuracy) against different k values and choose the k where the error stabilizes (elbow point).\n",
        "\n",
        "3.) **Grid Search:** Use automated hyperparameter tuning tools like GridSearchCV to search for the best k.\n",
        "\n",
        "4.) **Domain Knowledge:** Consider the problem context to decide whether a higher or lower k makes sense."
      ],
      "metadata": {
        "id": "GCbpOs0PIvs1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLEyI-UpJGMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
      ],
      "metadata": {
        "id": "kKNXCYELJJAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3.\n",
        "\n",
        "# Effect on Performance:\n",
        "\n",
        "Different distance metrics emphasize different aspects of the data.\n",
        "\n",
        "Metrics like Euclidean distance work well when the data is normalized and relationships are geometric.\n",
        "\n",
        "Manhattan distance is preferable for high-dimensional data or when features are on different scales.\n",
        "\n",
        "\n",
        "# Situations:\n",
        "\n",
        "1.) **Euclidean Distance:**\n",
        "\n",
        "Use for continuous, smooth data.\n",
        "\n",
        "Sensitive to feature scaling and outliers, so normalization is critical.\n",
        "\n",
        "2.) **Manhattan Distance:**\n",
        "\n",
        "Use for sparse or high-dimensional datasets.\n",
        "\n",
        "Less affected by outliers or irrelevant features.\n",
        "\n",
        "3.) **Other Metrics:**\n",
        "\n",
        "**Minkowski Distance:** Generalization of both Euclidean and Manhattan (use with different powers).\n",
        "\n",
        "4.) **Cosine Similarity:** Use when angles or relative proportions matter more than magnitude."
      ],
      "metadata": {
        "id": "l7zBjaLjJNNE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js40p9dIJnIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
      ],
      "metadata": {
        "id": "_H6tDgxpJv-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4.\n",
        "\n",
        "# Common Hyperparameters:\n",
        "\n",
        "1.) **k (Number of Neighbors):**\n",
        "\n",
        "Determines how many neighbors influence the prediction.\n",
        "\n",
        "Low k: Risk of overfitting.\n",
        "\n",
        "High k: Risk of underfitting.\n",
        "\n",
        "\n",
        "2.) **Distance Metric:**\n",
        "\n",
        "Determines how neighbors are identified.\n",
        "\n",
        "Common choices: Euclidean, Manhattan, Minkowski.\n",
        "\n",
        "\n",
        "3.) **Weighting Scheme:**\n",
        "\n",
        "**Uniform:** All neighbors contribute equally.\n",
        "\n",
        "**Distance-based:** Closer neighbors have higher influence.\n",
        "\n",
        "# Tuning Hyperparameters:\n",
        "\n",
        "**Grid Search/Random Search:** Explore combinations of k, distance metrics, and weighting schemes.\n",
        "\n",
        "**Cross-Validation:** Use k-fold cross-validation to evaluate performance across different hyperparameters.\n",
        "\n",
        "**Automated Tuning:** Tools like GridSearchCV or Bayesian optimization."
      ],
      "metadata": {
        "id": "AGLAN-TgJzSe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOulxkvwKS9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
      ],
      "metadata": {
        "id": "FlepQBewKWtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5.\n",
        "\n",
        "# Impact of Training Set Size:\n",
        "\n",
        "Larger datasets improve KNN performance as more neighbors provide a better representation of the data distribution.\n",
        "\n",
        "Very large datasets may increase computational cost and degrade performance in high-dimensional spaces.\n",
        "\n",
        "\n",
        "# Techniques to Optimize Training Set Size:\n",
        "\n",
        "1.) **Feature Selection:** Remove irrelevant features to reduce dimensionality.\n",
        "\n",
        "2.) **Sampling:** Use techniques like stratified sampling or undersampling to create representative subsets.\n",
        "\n",
        "3.) **Dimensionality Reduction:** Apply PCA or t-SNE to reduce the dataset size while retaining meaningful information.\n",
        "\n",
        "4.) **Approximate Nearest Neighbors (ANN):** Use algorithms like KD-Trees or Ball Trees for faster neighbor searches.\n"
      ],
      "metadata": {
        "id": "GjzCJi2OKY6t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJK8tnmdKYTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
      ],
      "metadata": {
        "id": "5a-d3BpUKq9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6.\n",
        "\n",
        "# Drawbacks:\n",
        "\n",
        "1.) **Computational Complexity:**\n",
        "\n",
        "KNN requires storing the entire training dataset and computing distances for each prediction, which can be computationally expensive.\n",
        "\n",
        "**Solution:** Use Approximate Nearest Neighbors (ANN) algorithms or indexing structures like KD-Trees.\n",
        "\n",
        "2.) **Sensitive to Irrelevant Features:**\n",
        "\n",
        "Irrelevant or noisy features can distort distance calculations.\n",
        "**Solution:** Perform feature selection or dimensionality reduction.\n",
        "\n",
        "3.) **Feature Scaling:**\n",
        "\n",
        "Distance metrics can be dominated by features with larger scales.\n",
        "\n",
        "**Solution:** Apply normalization or standardization.\n",
        "\n",
        "4.) **Curse of Dimensionality:**\n",
        "\n",
        "High-dimensional data makes neighbors appear equidistant.\n",
        "\n",
        "**Solution:** Use dimensionality reduction techniques like PCA or LDA.\n",
        "\n",
        "\n",
        "5.) **Imbalanced Data:**\n",
        "\n",
        "KNN struggles when classes are imbalanced, as neighbors from the majority class dominate.\n",
        "Solution: Use weighted KNN or balance the dataset with oversampling/undersampling.\n",
        "\n",
        "6.) **Overfitting to Noise:**\n",
        "\n",
        "Small values of k make KNN sensitive to noise.\n",
        "Solution: Choose an optimal k value using cross-validation.\n",
        "\n",
        "7.) **Storage Requirement:**\n",
        "\n",
        "Storing the entire dataset is memory-intensive.\n",
        "\n",
        "**Solution:** Reduce the dataset size using clustering or sampling techniques."
      ],
      "metadata": {
        "id": "La0kj54pKuRy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HN2GMLEjLcvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcJpILLrLhIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}