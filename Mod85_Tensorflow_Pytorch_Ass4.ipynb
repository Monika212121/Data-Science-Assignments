{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 85 Forward and Backward Propagation Assignment: 4"
      ],
      "metadata": {
        "id": "i0LDmHiwwtME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the purpose of forward propagation in a neural network?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pD3TvCWHwKsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Forward propagation is the process of passing input data through the network to generate an output. It involves:\n",
        "\n",
        "1.) Computing weighted sums of inputs.\n",
        "\n",
        "2.) Applying activation functions.\n",
        "\n",
        "3.) Producing predictions at the output layer.\n",
        "\n",
        "## Purpose:\n",
        "\n",
        "1.) Helps make predictions based on input data.\n",
        "\n",
        "2.) Lays the foundation for calculating errors in the training process."
      ],
      "metadata": {
        "id": "AU8lrivAxMhU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYDPcUZNxUMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?"
      ],
      "metadata": {
        "id": "ur8kZTBrxE7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Mathematically, forward propagation follows this process:\n",
        "\n",
        "1. Compute the weighted sum for each neuron:\n",
        "\n",
        "```\n",
        "z = W * X + b\n",
        "\n",
        "where:\n",
        "W = Weights\n",
        "X = Input features\n",
        "b = Bias\n",
        "```\n",
        "\n",
        "2. Apply an activation function: ``` a = f(z)```\n",
        "\n",
        "where f is a non-linear activation function like ReLU, Sigmoid, or Tanh.\n",
        "\n",
        "3. Compute the final output: ```Ypred = a```\n",
        "\n",
        "This process continues layer by layer in deep networks.\n"
      ],
      "metadata": {
        "id": "C4_ThwgcxXVG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpOMOKIwyHpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How are activation functions used during forward propagation?"
      ],
      "metadata": {
        "id": "lTS4pKeBxCsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.\n",
        "They are applied after computing the weighted sum of inputs.\n",
        "\n",
        "## Examples of activation functions:\n",
        "\n",
        "1. **ReLU:** ```f(x) = max(0, x) ```(used in hidden layers).\n",
        "\n",
        "2. **Sigmoid:** f(x) = 1 / (1 + e^-x) (used in binary classification).\n",
        "\n",
        "3. **Softmax:** Converts outputs into probabilities for multi-class classification.\n"
      ],
      "metadata": {
        "id": "siiyijvQyIee"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03_c7eseysVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the role of weights and biases in forward propagation?"
      ],
      "metadata": {
        "id": "3zD8AwKpxAxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4.\n",
        "\n",
        "## Weights (W):\n",
        "\n",
        "1. Represent the importance of input features.\n",
        "\n",
        "2. Adjusted during training to minimize error.\n",
        "\n",
        "## Biases (b):\n",
        "\n",
        "1. Allow the activation function to shift, making the model more flexible.\n",
        "\n",
        "2. Helps the network learn patterns even when inputs are zero."
      ],
      "metadata": {
        "id": "StmMZfRKytIH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYfr-19k2cfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?"
      ],
      "metadata": {
        "id": "51F2EaFKw-WT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. The Softmax function converts raw scores (logits) into probabilities that sum to 1:\n",
        "\n",
        "```\n",
        "σ(x) = e^xi / ∑(j=1 to N) e^xj\n",
        "```\n",
        "\n",
        "## Purpose:\n",
        "\n",
        "1. Converts output into probabilities, making interpretation easier.\n",
        "\n",
        "2. Ensures the model assigns a single most likely class in multi-class classification tasks.\n"
      ],
      "metadata": {
        "id": "VRRei2nly7ge"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dX1R3m_pzbBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the purpose of backward propagation in a neural network?"
      ],
      "metadata": {
        "id": "Wkhsvfwrw8rY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. Backward propagation (backpropagation) is used to adjust weights and biases by minimizing the error between predictions and actual values.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. **Compute Loss:** Compare prediction with actual values using a loss function (e.g., Cross-Entropy Loss, MSE).\n",
        "\n",
        "2. **Calculate Gradients:** Find how much each weight contributes to the error using differentiation.\n",
        "\n",
        "3. **Update Weights:** Use optimization algorithms (like Gradient Descent) to update weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "I3LZAZrxzbmb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tITNTatUzlaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?"
      ],
      "metadata": {
        "id": "-URinL2Rw7AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Backward propagation mathematically calculated in a single-layer feedforward neural network in following steps :    \n",
        "\n",
        "1. Compute the loss using a loss function:\n",
        "\n",
        "```\n",
        "L = [(Ytrue - Ypred)^2] / 2            \n",
        "```\n",
        "\n",
        "for Mean Squared Error (MSE).\n",
        "\n",
        "2. Compute gradients using derivatives:\n",
        "\n",
        "```\n",
        "∂W/∂L = ∂a/∂L * ∂z/∂a * ∂W/∂z\n",
        "```\n",
        "\n",
        "3. Update weights using Gradient Descent:\n",
        "\n",
        "```\n",
        "W = W - η * ∂W/∂L​\n",
        "\n",
        "```\n",
        "where **η** is the learning rate."
      ],
      "metadata": {
        "id": "s6ZXD919zrB5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iNqTTB70wKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Can you explain the concept of the chain rule and its application in backward propagation?"
      ],
      "metadata": {
        "id": "p_yC3cKzw4_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. The chain rule is a calculus principle that helps compute derivatives of composite functions:\n",
        "\n",
        "```\n",
        "dy/dx = dy/du * du/dx\n",
        "```\n",
        "\n",
        "In backpropagation, we apply the chain rule to propagate errors backward:\n",
        "\n",
        "```\n",
        "∂W/∂L = ∂L/ ∂a * ∂a/∂z * ∂z/∂W\n",
        "```\n",
        "\n",
        "## Why use the chain rule?\n",
        "\n",
        "1. It allows us to distribute the gradient backward through multiple layers.\n",
        "\n",
        "2. Helps update weights correctly in deep networks."
      ],
      "metadata": {
        "id": "y3Ht07Zl1ABH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hd7Ga8000_qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?"
      ],
      "metadata": {
        "id": "KspkGBflw25v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9. Some common challenges that can occur during backward propagation, and their solution are:\n",
        "\n",
        "1. **Vanishing Gradient Problem** (Gradients become too small in deep networks)\n",
        "\n",
        "Solution: Use ReLU or Leaky ReLU instead of Sigmoid/Tanh.\n",
        "\n",
        "2. **Exploding Gradient Problem** (Large gradients cause unstable training)\n",
        "\n",
        "Solution: Use Gradient Clipping to cap the gradient values.\n",
        "\n",
        "3. **Overfitting (Model learns noise instead of patterns)**\n",
        "\n",
        "Solution: Use Regularization (L1/L2), Dropout, and more data.\n",
        "\n",
        "4. **Slow convergence**\n",
        "\n",
        "Solution: Use adaptive optimizers like Adam, RMSprop, and increase batch size.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vg-QxGWc1vPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVaoolkPwKHv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7kIFWNXxGRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}