{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module71 Boosting Assignment2"
      ],
      "metadata": {
        "id": "iXYVHl-8JO_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?"
      ],
      "metadata": {
        "id": "gRxk210lJSwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. Gradient Boosting Regression is an ensemble learning method for regression tasks. It builds an additive model in a sequential manner, combining multiple weak learners (e.g., shallow decision trees). Each weak learner corrects the errors made by its predecessors by minimizing a specified loss function (e.g., Mean Squared Error) using gradient descent.\n",
        "\n",
        "# Key features include:\n",
        "\n",
        "1.) **Loss Function:** Optimizes a differentiable loss function (e.g., Mean Squared Error for regression).\n",
        "\n",
        "2.) **Sequential Training:** Each subsequent model learns to correct the residual errors from the previous ones.\n",
        "\n",
        "3.) **Weights:** Adjusts predictions iteratively to improve accuracy.\n"
      ],
      "metadata": {
        "id": "ugRGwfXcJU8x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QA-Y75djJQFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy."
      ],
      "metadata": {
        "id": "Qk1iAqKNJn3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Here's a Python implementation of Gradient Boosting Regression from scratch:"
      ],
      "metadata": {
        "id": "PLinKh4OJpaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Dataset (simple regression problem)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1.2, 2.8, 3.6, 4.5, 5.1])\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.model_weights = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize with the mean of the target\n",
        "        self.models.append(np.mean(y))\n",
        "        residuals = y - self.models[0]\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Fit a simple decision stump (shallow tree) on residuals\n",
        "            tree = DecisionStump(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "\n",
        "            # Update residuals\n",
        "            residuals -= self.learning_rate * predictions\n",
        "\n",
        "            # Store the tree\n",
        "            self.models.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Start with the initial mean\n",
        "        predictions = np.full(X.shape[0], self.models[0])\n",
        "        for tree in self.models[1:]:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "        return predictions\n",
        "\n",
        "class DecisionStump:\n",
        "    def __init__(self, max_depth=2):\n",
        "        self.threshold = None\n",
        "        self.feature_index = None\n",
        "        self.left_value = None\n",
        "        self.right_value = None\n",
        "\n",
        "    def fit(self, X, residuals):\n",
        "        best_mse = float(\"inf\")\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_index] <= threshold\n",
        "                right_mask = X[:, feature_index] > threshold\n",
        "\n",
        "                left_residuals = residuals[left_mask]\n",
        "                right_residuals = residuals[right_mask]\n",
        "\n",
        "                # Calculate the mean squared error\n",
        "                mse = (\n",
        "                    (left_residuals ** 2).sum() + (right_residuals ** 2).sum()\n",
        "                ) / n_samples\n",
        "\n",
        "                if mse < best_mse:\n",
        "                    best_mse = mse\n",
        "                    self.threshold = threshold\n",
        "                    self.feature_index = feature_index\n",
        "                    self.left_value = left_residuals.mean()\n",
        "                    self.right_value = right_residuals.mean()\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.full(X.shape[0], 0.0)\n",
        "        left_mask = X[:, self.feature_index] <= self.threshold\n",
        "        right_mask = X[:, self.feature_index] > self.threshold\n",
        "        predictions[left_mask] = self.left_value\n",
        "        predictions[right_mask] = self.right_value\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# Train the model\n",
        "gbr = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=2)\n",
        "gbr.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gbr.predict(X)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUkZc3U0JshP",
        "outputId": "7b5f46db-9eb7-4d16-e782-b808c1d26644"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.6907556678075526\n",
            "R-squared: 0.6298994493101411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZreHSfrJwvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best\n",
        "hyperparameters."
      ],
      "metadata": {
        "id": "k1HH245BJ1e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. Here’s how you can experiment with different hyperparameters like learning rate, number of trees, and tree depth in Gradient Boosting Regression using Grid Search and Random Search.\n",
        "\n",
        "We’ll use scikit-learn's GradientBoostingRegressor for simplicity and efficiency.\n",
        "\n",
        "1.) **Grid Search for Hyperparameter Tuning:**\n",
        "\n",
        "Grid Search involves searching exhaustively over a predefined set of hyperparameter combinations.\n"
      ],
      "metadata": {
        "id": "8vOfsOMhMhfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Dataset (example regression problem)\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.2, 2.8, 3.6, 4.5, 5.1])\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best parameters and performance\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\n",
        "print(\"R-squared:\", r2_score(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tSPm8RFMvEJ",
        "outputId": "8fe02287-9b11-4815-b4a1-e3eedec11c09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best Parameters: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 100}\n",
            "Mean Squared Error: 2.0490690472299029e-16\n",
            "R-squared: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.) **Random Search for Hyperparameter Tuning**\n",
        "\n",
        "Random Search randomly samples combinations of hyperparameters over a specified range. It is often faster than Grid Search."
      ],
      "metadata": {
        "id": "ndO1HPuJM0kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Hyperparameter distribution\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': uniform(0.01, 0.2),\n",
        "    'max_depth': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "# Random Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=gbr,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,  # Number of random combinations to try\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Best parameters and performance\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\n",
        "print(\"R-squared:\", r2_score(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctw0sirUM5TZ",
        "outputId": "ca070002-4ef8-496e-b841-477ca75a4469"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best Parameters: {'learning_rate': 0.13022300234864176, 'max_depth': 4, 'n_estimators': 200}\n",
            "Mean Squared Error: 2.01548754367405e-16\n",
            "R-squared: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.) **Explanation of Key Hyperparameters:**\n",
        "\n",
        "a.) **n_estimators:** The number of boosting stages (trees) to use. Higher values can improve accuracy but may lead to overfitting.\n",
        "\n",
        "b.) **learning_rate:** Controls the contribution of each tree. Lower values require more trees for accurate results but help prevent overfitting.\n",
        "\n",
        "c.) **max_depth:** Limits the depth of individual trees. Deeper trees capture more complexity but risk overfitting.\n",
        "\n",
        "4.) **Comparing Results:**\n",
        "\n",
        "After running both Grid Search and Random Search, compare the results and choose the method that:\n",
        "\n",
        "Finds the best hyperparameters faster.\n",
        "Produces a model with lower Mean Squared Error (MSE) and higher R²."
      ],
      "metadata": {
        "id": "4ZS_bJQvM979"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rw9Fx8HEKCyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a weak learner in Gradient Boosting?"
      ],
      "metadata": {
        "id": "KLAEBMQgKG8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. A weak learner is a model that performs slightly better than random guessing (e.g., a shallow decision tree or decision stump).\n",
        "\n",
        "In Gradient Boosting, multiple weak learners are combined to form a strong predictive model.\n",
        "\n",
        "Each weak learner focuses on correcting the errors of its predecessors."
      ],
      "metadata": {
        "id": "Dr89xYdDKJN4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsbMk6VLKMeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the intuition behind the Gradient Boosting algorithm?"
      ],
      "metadata": {
        "id": "57F-bSamKRh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. Gradient Boosting minimizes the error (loss function) by:\n",
        "\n",
        "1.) Training a weak learner to predict residual errors from the previous step.\n",
        "\n",
        "2.) Adding the weak learner's predictions to the ensemble in a way that reduces the overall loss.\n",
        "\n",
        "3.) Using gradient descent to optimize the model step by step.\n",
        "\n",
        "The idea is to move predictions closer to the true values by focusing on the hardest-to-predict examples."
      ],
      "metadata": {
        "id": "-zOmNhSeKSXx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kt1CHetnKc6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?"
      ],
      "metadata": {
        "id": "td2p8K7VKfXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. The Gradient Boosting algorithm build an ensemble of weak learners by following ways :\n",
        "\n",
        "1.) **Initialization:** Start with a base prediction (e.g., the mean of the target variable).\n",
        "\n",
        "2.) **Compute Residuals:** Calculate the residuals (errors) between actual values and predictions.\n",
        "\n",
        "3.) **Train Weak Learner:** Fit a weak learner to predict the residuals.\n",
        "\n",
        "4.) **Update Predictions:** Add the weak learner's predictions (scaled by a learning rate) to the ensemble.\n",
        "\n",
        "5.) **Repeat:** Iterate this process to gradually reduce residual errors.\n"
      ],
      "metadata": {
        "id": "ecNw1ZvpKgyW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8iSTyMSK4iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting?"
      ],
      "metadata": {
        "id": "NsdrRW2UK69U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. The steps involved in constructing the mathematical intuition of Gradient Boosting are -\n",
        "\n",
        "1.) **Define the Loss Function:**\n",
        "\n",
        "Choose a differentiable loss function L(y, f(x)) (e.g., Mean Squared Error).\n",
        "\n",
        "2.) **Compute the Negative Gradient:**\n",
        "\n",
        "Approximate the residuals as the negative gradient of the loss function:\n",
        "``` - ∂L / ∂f(xi) ```\n",
        "\n",
        "3.) **Fit Weak Learner:**\n",
        "\n",
        "Train a weak learner (e.g., decision tree) to predict the residuals (negative gradients).\n",
        "\n",
        "4.) **Update Model:**\n",
        "\n",
        "Add the weak learner's scaled predictions to the ensemble:\n",
        "``` f(x) <- f(x) + η * h(x) ``` , where η is the learning rate.\n",
        "\n",
        "5.) **Iterate:**\n",
        "\n",
        "Repeat steps 2-4 for a fixed number of iterations or until the error stops decreasing."
      ],
      "metadata": {
        "id": "XOzYzfRHK76R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wjx4NFcPMJR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "woPtMw4tNcHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}