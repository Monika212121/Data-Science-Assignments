{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module85 Activation function Assignment - 1"
      ],
      "metadata": {
        "id": "YX2vFBiap7jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an activation function in the context of artificial neural networks?"
      ],
      "metadata": {
        "id": "NiZ0CURApAuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1. An activation function is a mathematical function applied to the output of a neuron in a neural network.\n",
        "\n",
        "\n",
        "It introduces non-linearity to the network, enabling it to learn complex patterns and relationships in data.\n",
        "\n",
        "\n",
        "Without activation functions, a neural network would behave like a simple linear regression model.\n"
      ],
      "metadata": {
        "id": "pRUodoi5qJpw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92aSN63opZWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are some common types of activation functions used in neural networks?"
      ],
      "metadata": {
        "id": "xImzHpEipXlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. Some commonly used activation functions include:\n",
        "\n",
        "1. **Sigmoid** – S-shaped curve, used for binary classification.\n",
        "\n",
        "2. **Tanh (Hyperbolic Tangent)** – Similar to Sigmoid but ranges from -1 to 1.\n",
        "\n",
        "3. **ReLU (Rectified Linear Unit)** – Outputs 0 for negative inputs and x for positive inputs.\n",
        "\n",
        "4. **Leaky ReLU** – Modified ReLU that allows a small slope for negative values to avoid dead neurons.\n",
        "\n",
        "5. **Softmax** – Used for multi-class classification, outputs probability distribution.\n"
      ],
      "metadata": {
        "id": "u2-yjZsaqPLk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXQdtZf-paDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do activation functions affect the training process and performance of a neural network?"
      ],
      "metadata": {
        "id": "5O8aS6ZkpTAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. Activation functions play a crucial role in:\n",
        "\n",
        "1. **Learning complex patterns** – By introducing non-linearity, they help the network learn complex decision boundaries.\n",
        "\n",
        "2. **Gradient propagation** – Poor choices (like Sigmoid in deep networks) can lead to vanishing gradient problems, slowing training.\n",
        "\n",
        "3. **Training speed** – Functions like ReLU improve efficiency by reducing computational complexity.\n",
        "\n",
        "4. **Model accuracy** – The right activation function improves network performance in classification and regression tasks."
      ],
      "metadata": {
        "id": "l2XgpAtlqThl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6nUB7Q7fpa3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
      ],
      "metadata": {
        "id": "A8G2XX5DpQ6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. The Sigmoid function is defined as:\n",
        "\n",
        "```f(x) = 1 / (1 + e^-x)```\n",
        "\n",
        "It maps input values to a range of (0,1), making it useful for binary classification problems.\n",
        "\n",
        "✅ Advantages:\n",
        "\n",
        "1. Converts input into probabilities, making it interpretable.\n",
        "\n",
        "2. Smooth and differentiable.\n",
        "\n",
        "❌ Disadvantages:\n",
        "\n",
        "1. **Vanishing Gradient Problem** – Large negative or positive inputs lead to very small gradients, slowing training.\n",
        "\n",
        "2. **Not zero-centered** – Outputs are always positive, which can slow convergence.\n",
        "\n",
        "3. **Expensive computation** – Due to exponentiation."
      ],
      "metadata": {
        "id": "HIyZJ2_hqucW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qb714SMepbhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
      ],
      "metadata": {
        "id": "Pvre-uK_pOhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. The ReLU function is defined as:\n",
        "\n",
        "```f(x) = max(0, x)```\n",
        "\n",
        "It outputs 0 for negative values and the input itself for positive values.\n",
        "\n",
        "## Differences from Sigmoid:\n",
        "\n",
        "1. **No exponentiation** → ReLU is computationally efficient.\n",
        "\n",
        "2. **Prevents vanishing gradient** → Unlike Sigmoid, large positive inputs maintain strong gradients.\n",
        "\n",
        "3. **Non-saturating** → Helps deeper networks train faster.\n",
        "\n",
        "4. **Dead Neurons Problem** → If many neurons receive negative inputs, they output zero and stop learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "eeAeOHDjrLTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mjzzkawpcGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
      ],
      "metadata": {
        "id": "fBrdwgh-pMUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6. Benefits of using the ReLU activation function over the sigmoid function are:-\n",
        "\n",
        "1. **Avoids vanishing gradients** – Keeps gradients large for positive values, helping deeper networks learn efficiently.\n",
        "\n",
        "2. **Computationally efficient** – Requires only a max operation, unlike sigmoid which involves exponentiation.\n",
        "\n",
        "3. **Sparse activation** – Many neurons output zero, leading to a more efficient and sparse network representation.\n",
        "\n",
        "4. **Faster convergence** – Leads to better gradient flow and faster optimization."
      ],
      "metadata": {
        "id": "waoVPeHtrmPY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFxox9r9pcwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
      ],
      "metadata": {
        "id": "I_1oUtaapJOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Leaky ReLU modifies ReLU to allow a small, non-zero slope (αx) for negative inputs:\n",
        "\n",
        "```\n",
        "f(x) = x, x > 0\n",
        "     = ax, x <= 0\n",
        "\n",
        "```\n",
        "where **a** is a small positive constatn(e.g., 0.01).\n",
        "\n",
        "## How it helps?\n",
        "\n",
        "1. Prevents neurons from becoming completely inactive (avoiding the dead neuron problem).\n",
        "\n",
        "2. Retains some gradient flow even for negative inputs, unlike ReLU.\n",
        "\n",
        "3. Helps in better gradient propagation in deeper networks."
      ],
      "metadata": {
        "id": "rNgGHqTqr4Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ek6t15kCpegJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the purpose of the softmax activation function? When is it commonly used?"
      ],
      "metadata": {
        "id": "d625CEFzpG45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. The Softmax function converts a vector of real numbers into probabilities that sum to 1:\n",
        "```\n",
        "σ(x) = e^xi / ∑(j=1 to N) e^xj\n",
        "\n",
        "```\n",
        "where, N = Total classes\n",
        "\n",
        "It is commonly used in the output layer of multi-class classification problems, where each class is assigned a probability.\n",
        "\n",
        "## Why use Softmax?\n",
        "\n",
        "1. Converts logits into probabilities.\n",
        "\n",
        "2. Ensures that the sum of all outputs is 1 (useful for classification).\n",
        "\n",
        "3. Allows easy interpretation of class predictions.\n",
        "\n",
        "## Where is it used?\n",
        "\n",
        "Last layer of a neural network for multi-class classification (e.g., image classification).\n"
      ],
      "metadata": {
        "id": "3UrJPdZMsb-x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oaha_62SpGWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
      ],
      "metadata": {
        "id": "mTIYEARepD9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9. The tanh function is defined as:\n",
        "\n",
        "```\n",
        "f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "```\n",
        "\n",
        "It maps input values to the range (-1,1).\n",
        "\n",
        "## Comparison with Sigmoid:\n",
        "```\n",
        "Feature\t             Sigmoid\t             Tanh\n",
        "----------------------------------------------------------------------------------------------\n",
        "Range\t                (0,1)\t              (-1,1)\n",
        "Zero-centered?\t       No             \t   Yes\n",
        "Saturation issue?\t     Yes\t               Yes\n",
        "Preferred for?\tBinary classification\t   Hidden layers in deep networks\n",
        "```\n",
        "\n",
        "## Why prefer Tanh over Sigmoid?\n",
        "\n",
        "1. Zero-centered, leading to better weight updates.\n",
        "\n",
        "2. Stronger gradient for negative values, reducing vanishing gradient risk.\n",
        "\n",
        "## Disadvantage:\n",
        "\n",
        "Still suffers from vanishing gradients in deep networks.\n",
        "\n"
      ],
      "metadata": {
        "id": "zYub0izWtK_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2xWKwPAneQN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIV1YMzFpf5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}