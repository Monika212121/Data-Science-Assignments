{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module29 Web scrapping Assignment"
      ],
      "metadata": {
        "id": "QnT3ztBcHZDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
      ],
      "metadata": {
        "id": "66GRCnoCHLXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1.\n",
        "\n",
        "### What is Web Scraping?\n",
        "Web Scraping is an automated technique used to extract data from websites. It involves sending requests to web pages, extracting relevant information, and saving it in a structured format (CSV, JSON, database, etc.).\n",
        "\n",
        "\n",
        "### Why is Web Scraping Used?\n",
        "1.) To collect large amounts of data quickly.\n",
        "\n",
        "2.) To automate data extraction from dynamic websites.\n",
        "\n",
        "3.) To gather real-time or historical data for analysis.\n",
        "\n",
        "\n",
        "### Three Areas Where Web Scraping is Used:\n",
        "\n",
        "**1. E-commerce Price Monitoring**\n",
        "\n",
        "a.) Scraping product prices from Amazon, Flipkart, etc.\n",
        "\n",
        "b.) Helps businesses compare competitor pricing.\n",
        "\n",
        "\n",
        "**2. Job Market Analysis**\n",
        "\n",
        "a.) Extracting job listings from LinkedIn, Indeed, etc.\n",
        "\n",
        "b.) Helps in tracking trends and salary insights.\n",
        "\n",
        "\n",
        "**3. News & Sentiment Analysis**\n",
        "\n",
        "a.) Scraping news articles from CNN, BBC, etc.\n",
        "\n",
        "b.) Used for stock market predictions and social media trends.\n"
      ],
      "metadata": {
        "id": "PDijsd76Hyfn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUZuDITrHOaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the different methods used for Web Scraping?"
      ],
      "metadata": {
        "id": "NBh838K3HNhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2. The different methods for web scrapping are :\n",
        "\n",
        "**1. Using requests Library (Basic Method)**\n",
        "\n",
        "a.) Sends HTTP requests to fetch web page content.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "qt3vPfRwIpKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get('https://example.com')\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "vGZKo7ddI2Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Using BeautifulSoup (Parsing HTML)**\n",
        "\n",
        "a.) Parses and extracts data from HTML content.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "nfx2uSc2I6NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(soup.title.text)\n"
      ],
      "metadata": {
        "id": "-QNPfC3bHQoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Using Selenium (For Dynamic Pages)**\n",
        "\n",
        "a.) Automates browser actions for JavaScript-based pages.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "ILDkJxjuJIDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "driver = webdriver.Chrome()\n",
        "driver.get('https://example.com')\n",
        "print(driver.page_source)\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "STZtHFqjJFJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Using Scrapy (Advanced Web Scraping Framework)**\n",
        "\n",
        "a.) Handles large-scale scraping projects efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "iwvFwV_2JRlp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9L9Fo5GJEuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Beautiful Soup? Why is it used?"
      ],
      "metadata": {
        "id": "8W_nTEVBHRE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3.\n",
        "\n",
        "### What is BeautifulSoup?\n",
        "a.) BeautifulSoup is a Python library used to parse and extract data from HTML/XML files.\n",
        "\n",
        "b.) It works with requests to navigate, search, and modify HTML.\n",
        "\n",
        "### Why is BeautifulSoup Used?\n",
        "a.) Easier HTML Parsing: Extracts content without writing complex regex.\n",
        "\n",
        "b.) Supports Multiple Parsers: Works with html.parser, lxml, etc.\n",
        "\n",
        "c.) Efficient Data Extraction: Finds elements by tag, class, or ID.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "bc3cYJEeJahJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "html_code = \"<html><body><h1>Hello, World!</h1></body></html>\"\n",
        "soup = BeautifulSoup(html_code, 'html.parser')\n",
        "print(soup.h1.text)  # Output: Hello, World!\n"
      ],
      "metadata": {
        "id": "exZOf7geJo7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acIFGXEHHRso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is flask used in this Web Scraping project?"
      ],
      "metadata": {
        "id": "uY6yVmqiHUIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4. Flask is used in a web scraping project to serve scraped data as an API or web application.\n",
        "\n",
        "### Why Use Flask?\n",
        "1.) Lightweight & Easy to Use – Minimal setup required.\n",
        "\n",
        "2.) Creates APIs to Serve Scraped Data – Provides structured data to front-end or other applications.\n",
        "\n",
        "3.) Allows User Interaction – Users can input queries (e.g., search terms for scraping).\n",
        "\n",
        "4.) Enables Deployment – Host web scraping results on a local/remote server.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "GrnNJ0RmJteZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, jsonify\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/data')\n",
        "def scraped_data():\n",
        "    return jsonify({\"product\": \"Laptop\", \"price\": \"$999\"})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "iMS7GdSAJ54N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates an API that returns scraped data."
      ],
      "metadata": {
        "id": "xJsTMKCGJ9Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yB9TQ6aCHTbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
      ],
      "metadata": {
        "id": "xLLuEi9HHWuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5.\n",
        "\n",
        "### AWS Services Used in Web Scraping:\n",
        "\n",
        "**1. Amazon EC2 (Elastic Compute Cloud)**\n",
        "\n",
        "a.) Provides virtual servers to run the Flask web scraping application.\n",
        "\n",
        "b.) Handles large-scale scraping jobs efficiently.\n",
        "\n",
        "\n",
        "**2. Amazon S3 (Simple Storage Service)**\n",
        "\n",
        "a.) Stores scraped data (CSV, JSON, images).\n",
        "\n",
        "b.) Used for data backup and retrieval.\n",
        "\n",
        "\n",
        "**3. Amazon RDS (Relational Database Service)**\n",
        "\n",
        "a.) Stores structured scraped data in MySQL/PostgreSQL.\n",
        "\n",
        "b.) Enables easy querying and analysis.\n",
        "\n",
        "**4. AWS Lambda**\n",
        "\n",
        "a.) Runs serverless web scraping scripts on a schedule.\n",
        "\n",
        "b.) Reduces costs by running only when needed.\n",
        "\n",
        "**5. Amazon CloudWatch**\n",
        "\n",
        "a.) Monitors logs and performance of scraping tasks.\n",
        "\n",
        "b.) Helps in debugging errors.\n",
        "\n",
        "## Example Workflow in AWS:\n",
        "\n",
        "1.) EC2 runs a Flask app to serve scraped data.\n",
        "\n",
        "2.) Scraped data is stored in S3 or RDS.\n",
        "\n",
        "3.) Lambda triggers the scraper periodically.\n",
        "\n",
        "4.) CloudWatch monitors logs and alerts errors.\n"
      ],
      "metadata": {
        "id": "mSbqsrXvKCnR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utKANR38HXTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2P3lp2xvK3Jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}